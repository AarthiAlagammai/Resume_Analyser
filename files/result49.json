"{\"res_summary\": [\" \" 7+ years of professional experience in Statistical modeling, Machine Learning, Data Visualization.n \" Expertise in transforming business requirements into building models, designing algorithms, developing data mining and reporting solutions that scales across massive volume of unstructured data and structured.n \" Extensive experience in Text Analytics, developing different Statistical Machine Learning, Data Mining solutions to various business problems and generating Data Visualizations using R, Python and Tableau.n \" Experience in developing different Statistical Machine Learning, Text Analytics, Data Mining solutions to various business problems and data visualizations using R and Python.n \" Experience in integrating data, profiling, validating and data cleansing transformation and data visualization using R, SAS and Python.n \" Extensive working experience with Python Libraries including Scikit-learn, Pandas, Numpy, H20 and Pyspark.n \" Hands on experience in implementing LDA, Naive Bayes and skilled in Random Forests, Decision Trees, Linear and Logistic Regression, SVM, Clustering, Neural Networks, Principle Component Analysis(PCA), K-Means and good knowledge on Recommender Systems.n \" Proficient in Statistical Modeling and Machine Learning techniques (Linear, Logistics, Decision Trees, Random Forest, SVM, K-NearestNeighbors, Bayesian, XG Boost) in Forecasting/ Predictive Analytics, Segmentation methodologies, Regression based models, Hypothesis testing, time series analysis, Factor analysis, Ensembles.n \" Expertise in Natural Language Processing (NLP) like Sentiment Analysis, Text Analytics.n \" Proficient in Data visualization tools such as Tableau, Plotly, Python Matplotlib and Seaborn.n \" Data Acquisition, Data Preparation, Data Manipulation, Feature Engineering, Statistical Modeling, Testing and Validation, Visualization and Reporting.n \" Involved in the entire data science project life cycle and actively involved in all the phases including data extraction, data cleaning, statistical modelling, and data visualization with large data sets of structured and unstructured data in Agile/SCRUM environment.n \" Proficient in Statistical Methods like Parametric and Non- Parametric tests, ANOVA test, T-test, Chi-Square Test and 2 Proportion Test.n \" 4+ years of experience in providing end-to-end analytics solutions across Retail, E-commerce, Human Resources and Analytics Product Development industries.n TOOLS AND TECHNOLOGIES:n n Programming Languages:n Python: CARET, glmnet, forecast, XG boost, Sci-kit learnn SAS: Forecast server, SAS Procedures and Data modelingn Spark: MLlibn SQL: Analytical & Windowing functions, Subqueries, joins, DDL/DML statementsn R: CARET, Random Forestn n Statistical Methodsn Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Neural Networks.n Unsupervised Learning: Principal Component Analysis(PCA), K-Means, Hierarchical Clustering, Market Basket Analysis, Collaborative Filtering and Low Rank Matrix Factorizationn Sampling Methods: Bootstrap sampling methods and Stratified samplingn Model Tuning/Selection: Cross Validation, AIC/BIC Criterions, Grid Search and Regularization, Dimension Reductionn n ETL/BI Tools Tableau, Advanced-Excel, ggplot2n Natural Language Processing/Text Miningn Document Term Matrix(DTM), Stemming, Lemmatization, Word Embedding, Semantic, Term Frequency, Dependency Parsing, Sentiment, Natural Language Generator(NLG), Word Cloud, Named Entity Recognition(NER), Parts Of Speech(POS) Taggingn n IDE Spyder, RStudio, Jupyter, Anaconda, H2O, Pyspark, Flask, Django, Docker, R shinyn Other: GIT, Statistics, Microsoft Azure, Google Cloud Platform, Amazon AWS, Hadoop\"], \"education\": [], \"work_experience\": [{\"title\": \"Data Scientist\", \"company\": \"GIIR America Inc\", \"work_dates\": \"October 2014 to January 2017\", \"description\": \" \" Extensively involved in all phases of data acquisition, data collection, data cleaning, model development, model validation, and visualization to deliver data science solutions.n \" Extracted data from database, copied into Hadoop Distributed File system(HDFS) and used Hadoop tools such as Hive and Pig Latin to retrieve the data required for building models.n \" Worked on data cleaning and ensured data quality, consistency, integrity using Pandas, NumPy.n \" Tackled highly imbalanced Fraud dataset using sampling techniques like down-sampling, up-sampling and SMOTE (Synthetic Minority Over-Sampling Technique) using Python Scikit-learn.n \" Used Principal Component Analysis(PCA) and other feature engineering techniques to reduce the high dimensional data, feature normalization techniques and label encoding with Scikit-learn library in Python.n \" Used Pandas, NumPy, Seaborn, Matplotlib, Scikit-learn in Python for developing various machine learning models such as Logistic Regression, Gradient Boost Decision Tree and Neural Network.n \" Used Cross-Validation(K-Fold) to test the models with different batches of data to optimize the models and prevent overfitting.n \" Experimented with Ensemble methods to increase the accuracy of the training model with different Bagging(Random Forest) and Boosting(Gradient Boosting) methods.n \" Implemented a Python-based distributed random forest via PySpark and MLlib.n \" Used AWS S3, DynamoDB, AWS lambda, AWS EC2 for data storage and models' deployment.n \" Created and maintained reports to display the status and performance of deployed model and algorithm with Tableau\"}, {\"title\": \"SR n DATASCIENCE\", \"company\": \"Capital one\", \"description\": \"Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, time, Date and Time etc. Integrating with external data sources and APIs to discover interesting trends.n Built Machine Learning models to identify fraudulent applications for loan pre-approvals and to identify fraudulent credit card transactions using the history of customer transactions with supervised learning methods.n Involved in various pre-processing phases of text data like Tokenizing, Stemming, Lemmatization and converting the raw text data to structured data.n Personalization, Target Marketing, Customer Segmentation and profiling.n Performed Data Cleaning, features scaling, featurization, features engineering.n Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model and utilized machine learning algorithms such as linear regression, Naive Bayes, Random Forests, Decision Trees, K-means, & KNN.n Implemented number of Natural Language process mechanism for Chat Bots.n Customer segmentation based on their behavior or specific characteristics like age, region, income, geographical location and applying Clustering algorithms to group the customers based on their similar behavior patterns.n The results from the segmentation helps to learn the Customer Lifetime Value of every segment and discover high value and low value segments and to improve the customer service to retain the customers.n Performed Clustering with historical, demographic and behavioral data as features to implement the Personalized marketing that offers right product to right person at the right time on the right device.n Implemented Principal Component Analysis(PCA) in feature engineering to analyze high dimensional data.n Addressed overfitting and underfitting by tuning the hyper parameter of the algorithm and by using L1 and L2 Regularization.n Used Spark's Machine learning library to build and evaluate different models\"}, {\"title\": \"Jr. Data Scientist\", \"company\": \"Blue Cross Bluenn -n nnShields, TX\", \"work_dates\": \"September 2012 to October 2014\", \"description\": \" \" Performed advanced and predictive data analytics using data science technology to predict either medical claim is legit or fraud by using very effective and power machine learning algorithms.n \" Trained models with 700K+ customer emails and tested with 93% and above accuracy on prediction.n \" Processed the Raw data from CSV files in to organized form by applying Data Cleaning techniques using Pandas and NumPy.n \" Performed analysis using Count Vectorizer, TF-IDF, Linear SVC and pipelined those to develop model to predict spam and classify which emails to respond back and which to put in the junk emails.n \" Built Confusion Matrix and Classification report to see the performance of the algorithm.n \" The jobs were made to run successfully by solving data quality issues using SQL, efficient coding practices, macros and stored procedures.\"}, {\"title\": \"Data Scientist\", \"company\": \"Capital One, VA\", \"work_dates\": \"February 2017 to Present\", \"description\": \" \" Performed Data Profiling to learn about behavior with various features such as traffic pattern, location, time, Date and Time etc. Integrating with external data sources and APIs to discover interesting trends.n \" Built Machine Learning models to identify fraudulent applications for loan pre-approvals and to identify fraudulent credit card transactions using the history of customer transactions with supervised learning methods.n \" Involved in various pre-processing phases of text data like Tokenizing, Stemming, Lemmatization and converting the raw text data to structured data.n \" Personalization, Target Marketing, Customer Segmentation and profiling.n \" Performed Data Cleaning, features scaling, featurization, features engineering.n \" Used Pandas, NumPy, SciPy, Matplotlib, Seaborn, Scikit-learn in Python at various stages for developing machine learning model and utilized machine learning algorithms such as linear regression, Naive Bayes, Random Forests, Decision Trees, K-means, & KNN.n \" Implemented number of Natural Language process mechanism for Chat Bots.n \" Customer segmentation based on their behavior or specific characteristics like age, region, income, geographical location and applying Clustering algorithms to group the customers based on their similar behavior patterns.n \" The results from the segmentation helps to learn the Customer Lifetime Value of every segment and discover high value and low value segments and to improve the customer service to retain the customers.n \" Performed Clustering with historical, demographic and behavioral data as features to implement the Personalized marketing that offers right product to right person at the right time on the right device.n \" Implemented Principal Component Analysis(PCA) in feature engineering to analyze high dimensional data.n \" Addressed overfitting and underfitting by tuning the hyper parameter of the algorithm and by using L1 and L2 Regularization.n \" Used Spark's Machine learning library to build and evaluate different models.\"}], \"links\": [], \"certifications\": [], \"awards\": [], \"skills\": [\"Python: CARET, glmnet, forecast, XG boost, Sci-kit learn SAS: Forecast server, SAS Procedures and Data modeling Spark: MLlib SQL: Analytical & Windowing functions, Subqueries, joins, DDL/DML statements R: CARET, Random Forest\", \"Document Term Matrix(DTM), Stemming, Lemmatization, Word Embedding, Semantic, Term Frequency, Dependency Parsing, Sentiment, Natural Language Generator(NLG), Word Cloud, Named Entity Recognition(NER), Parts Of Speech(POS) Tagging\", \"Supervised Learning: Linear/Logistic Regression, Lasso, Ridge, Decision Trees, Ensemble Methods, Random Forests, Support Vector Machines, Gradient Boosting, XGB, Neural Networks. Unsupervised Learning: Principal Component Analysis(PCA), K-Means, Hierarchical Clustering, Market Basket Analysis, Collaborative Filtering and Low Rank Matrix Factorization Sampling Methods: Bootstrap sampling methods and Stratified sampling Model Tuning/Selection: Cross Validation, AIC/BIC Criterions, Grid Search and Regularization, Dimension Reduction\"], \"additional_info\": [\"Operating Systems Windows, LINUX, Macintosh HD\"], \"publication\": []}"